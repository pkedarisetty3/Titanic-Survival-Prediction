{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic Survival Prediction","metadata":{}},{"cell_type":"markdown","source":"## Context","metadata":{}},{"cell_type":"markdown","source":"On April 15, 1912, the Titanic collided into an iceberg and sank. Out of 2224 passengers and crew on the ship, 1502 of them unfortunately died.","metadata":{}},{"cell_type":"markdown","source":"## Objective","metadata":{}},{"cell_type":"markdown","source":"The Titanic included many different types of people, which may have played a factor in whether or not each person survived. The object of this project is to create a model that can predict if someone survived the crash based on these various factors.","metadata":{}},{"cell_type":"markdown","source":"## Data Overview","metadata":{}},{"cell_type":"markdown","source":"Let's start by importing the data and various packages that will help with our analysis and modeling.\n\nGiven that we want to predict whether or not someone survived the Titanic crash, we will want to use classification modeling. We will explore both a logistic regression model and a decision tree model.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # data visualization\n\n# Removes the limit from the number of displayed columns and rows.\n# This is so I can see the entire dataframe when I print it\npd.set_option(\"display.max_columns\", None)\n# pd.set_option('display.max_rows', None)\npd.set_option(\"display.max_rows\", 200)\n\n# To filter the warnings\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# To build linear model for statistical analysis and prediction\nimport scipy.stats as stats\nimport statsmodels.stats.api as sms\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nfrom statsmodels.tools.tools import add_constant\n\n# Libraries to build decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\n\n# To tune different models\nfrom sklearn.model_selection import GridSearchCV\n\n# To get diferent metric scores\nfrom sklearn import metrics\nfrom sklearn.metrics import (\n    f1_score,\n    accuracy_score,\n    recall_score,\n    precision_score,\n    confusion_matrix,\n    plot_confusion_matrix,\n    make_scorer,\n    roc_curve,\n    roc_auc_score,\n    precision_recall_curve\n)\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-05T13:59:28.823697Z","iopub.execute_input":"2022-06-05T13:59:28.823950Z","iopub.status.idle":"2022-06-05T13:59:29.648436Z","shell.execute_reply.started":"2022-06-05T13:59:28.823881Z","shell.execute_reply":"2022-06-05T13:59:29.647586Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Import the train data and the test data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.649687Z","iopub.execute_input":"2022-06-05T13:59:29.649946Z","iopub.status.idle":"2022-06-05T13:59:29.677027Z","shell.execute_reply.started":"2022-06-05T13:59:29.649916Z","shell.execute_reply":"2022-06-05T13:59:29.676162Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.678537Z","iopub.execute_input":"2022-06-05T13:59:29.679506Z","iopub.status.idle":"2022-06-05T13:59:29.703123Z","shell.execute_reply.started":"2022-06-05T13:59:29.679425Z","shell.execute_reply":"2022-06-05T13:59:29.702078Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Let's see some quick observations we can make in the data.","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.707240Z","iopub.execute_input":"2022-06-05T13:59:29.707514Z","iopub.status.idle":"2022-06-05T13:59:29.725358Z","shell.execute_reply.started":"2022-06-05T13:59:29.707459Z","shell.execute_reply":"2022-06-05T13:59:29.724548Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.726907Z","iopub.execute_input":"2022-06-05T13:59:29.727604Z","iopub.status.idle":"2022-06-05T13:59:29.733618Z","shell.execute_reply.started":"2022-06-05T13:59:29.727571Z","shell.execute_reply":"2022-06-05T13:59:29.732687Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data.describe(include=\"all\").T","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.735008Z","iopub.execute_input":"2022-06-05T13:59:29.737088Z","iopub.status.idle":"2022-06-05T13:59:29.782327Z","shell.execute_reply.started":"2022-06-05T13:59:29.737039Z","shell.execute_reply":"2022-06-05T13:59:29.781432Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# checking for null values\ntrain_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.784022Z","iopub.execute_input":"2022-06-05T13:59:29.784538Z","iopub.status.idle":"2022-06-05T13:59:29.794164Z","shell.execute_reply.started":"2022-06-05T13:59:29.784494Z","shell.execute_reply":"2022-06-05T13:59:29.793450Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# checking for duplicate values\ntrain_data.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.795560Z","iopub.execute_input":"2022-06-05T13:59:29.796057Z","iopub.status.idle":"2022-06-05T13:59:29.809729Z","shell.execute_reply.started":"2022-06-05T13:59:29.796012Z","shell.execute_reply":"2022-06-05T13:59:29.809096Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"##### Observations:\n- We should remove the PassengerId and Name columns from the data before entering into the model. These variables are unique for every person, so it doesn't make sense to make predictions based off those variables.\n- There are no duplicate rows.\n- Out of 891 rows in the training data, there are 687 null values for the Cabin column. It probably doesn't make sense to make predictions off of a variable with not much information, so we should just remove that column.\n- There are 177 null values for the Age column. I will likely populate the null values with the median value of the Age column.\n- There are 2 null values for the Embarked column. I may either keep that as is or populate with the mode value of the Embarked column.\n- The Pclass column is currently an int column. This is potentially ok because there is a quantifiable difference between the ticket classes (e.g. 1st is the highest class, 3rd is the lowest class). However, it may be valuable to convert this column to a category column and then one-hot encode it.\n- The Fare column will likely have multicollinearity with the Pclass column, because the passenger fare is likely higher for 1st class compared to other classes.\n- We should delete the Ticket column. The ticket number value doesn't provide quantifiable difference between rows, and there are 681 unique Ticket column values. Any row that shares ticket numbers are likely in the same party and will be considered from the Sibsp and Parch columns.\n- Overall, there is definitely some data cleaning to do. We will do Exploratory Data Analysis to confirm if other data cleaning should occur.","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis & Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Remove the PassengerId, Name, Ticket, and Cabin columns\ncols = [\"PassengerId\",\"Name\",\"Ticket\", \"Cabin\"]\nfor col in cols:\n    train_data = train_data.drop([col], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.810659Z","iopub.execute_input":"2022-06-05T13:59:29.811249Z","iopub.status.idle":"2022-06-05T13:59:29.822429Z","shell.execute_reply.started":"2022-06-05T13:59:29.811215Z","shell.execute_reply":"2022-06-05T13:59:29.821525Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.823621Z","iopub.execute_input":"2022-06-05T13:59:29.824258Z","iopub.status.idle":"2022-06-05T13:59:29.841340Z","shell.execute_reply.started":"2022-06-05T13:59:29.824223Z","shell.execute_reply":"2022-06-05T13:59:29.840447Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#### Creating a couple functions to help with visualization:","metadata":{}},{"cell_type":"code","source":"# function to plot a boxplot and a histogram along the same scale.\n\n\ndef histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n    \"\"\"\n    Boxplot and histogram combined\n\n    data: dataframe\n    feature: dataframe column\n    figsize: size of figure (default (12,7))\n    kde: whether to the show density curve (default False)\n    bins: number of bins for histogram (default None)\n    \"\"\"\n    f2, (ax_box2, ax_hist2) = plt.subplots(\n        nrows=2,  # Number of rows of the subplot grid= 2\n        sharex=True,  # x-axis will be shared among all subplots\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )  # creating the 2 subplots\n    sns.boxplot(\n        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n    )  # boxplot will be created and a star will indicate the mean value of the column\n    sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n    ) if bins else sns.histplot(\n        data=data, x=feature, kde=kde, ax=ax_hist2\n    )  # For histogram\n    ax_hist2.axvline(\n        data[feature].mean(), color=\"green\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax_hist2.axvline(\n        data[feature].median(), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.842613Z","iopub.execute_input":"2022-06-05T13:59:29.842993Z","iopub.status.idle":"2022-06-05T13:59:29.852775Z","shell.execute_reply.started":"2022-06-05T13:59:29.842960Z","shell.execute_reply":"2022-06-05T13:59:29.851965Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# function to create labeled barplots\n\n\ndef labeled_barplot(data, feature, perc=False, n=None):\n    \"\"\"\n    Barplot with percentage at the top\n\n    data: dataframe\n    feature: dataframe column\n    perc: whether to display percentages instead of count (default is False)\n    n: displays the top n category levels (default is None, i.e., display all levels)\n    \"\"\"\n\n    total = len(data[feature])  # length of the column\n    count = data[feature].nunique()\n    if n is None:\n        plt.figure(figsize=(count + 2, 6))\n    else:\n        plt.figure(figsize=(n + 2, 6))\n\n    plt.xticks(rotation=90, fontsize=15)\n    ax = sns.countplot(\n        data=data,\n        x=feature,\n        palette=\"Paired\",\n        order=data[feature].value_counts().index[:n],\n    )\n\n    for p in ax.patches:\n        if perc == True:\n            label = \"{:.1f}%\".format(\n                100 * p.get_height() / total\n            )  # percentage of each class of the category\n        else:\n            label = p.get_height()  # count of each level of the category\n\n        x = p.get_x() + p.get_width() / 2  # width of the plot\n        y = p.get_height()  # height of the plot\n\n        ax.annotate(\n            label,\n            (x, y),\n            ha=\"center\",\n            va=\"center\",\n            size=12,\n            xytext=(0, 5),\n            textcoords=\"offset points\",\n        )  # annotate the percentage\n\n    plt.show()  # show the plot","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.854027Z","iopub.execute_input":"2022-06-05T13:59:29.854588Z","iopub.status.idle":"2022-06-05T13:59:29.870934Z","shell.execute_reply.started":"2022-06-05T13:59:29.854552Z","shell.execute_reply":"2022-06-05T13:59:29.870049Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def stacked_barplot(data, predictor, target):\n    \"\"\"\n    Print the category counts and plot a stacked bar chart\n\n    data: dataframe\n    predictor: independent variable\n    target: target variable\n    \"\"\"\n    count = data[predictor].nunique()\n    sorter = data[target].value_counts().index[-1]\n    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n        by=sorter, ascending=False\n    )\n    print(tab1)\n    print(\"-\" * 120)\n    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n        by=sorter, ascending=False\n    )\n    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n    plt.legend(\n        loc=\"lower left\", frameon=False,\n    )\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.875593Z","iopub.execute_input":"2022-06-05T13:59:29.876061Z","iopub.status.idle":"2022-06-05T13:59:29.886789Z","shell.execute_reply.started":"2022-06-05T13:59:29.876014Z","shell.execute_reply":"2022-06-05T13:59:29.885865Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"### function to plot distributions wrt target\n\n\ndef distribution_plot_wrt_target(data, predictor, target):\n\n    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n\n    target_uniq = data[target].unique()\n\n    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[0]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 0],\n        color=\"teal\",\n        stat=\"density\",\n    )\n\n    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n    sns.histplot(\n        data=data[data[target] == target_uniq[1]],\n        x=predictor,\n        kde=True,\n        ax=axs[0, 1],\n        color=\"orange\",\n        stat=\"density\",\n    )\n\n    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n\n    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n    sns.boxplot(\n        data=data,\n        x=target,\n        y=predictor,\n        ax=axs[1, 1],\n        showfliers=False,\n        palette=\"gist_rainbow\",\n    )\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.888001Z","iopub.execute_input":"2022-06-05T13:59:29.888662Z","iopub.status.idle":"2022-06-05T13:59:29.903675Z","shell.execute_reply.started":"2022-06-05T13:59:29.888628Z","shell.execute_reply":"2022-06-05T13:59:29.902913Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"##### What is the age distribution across the people on the Titanic?","metadata":{}},{"cell_type":"code","source":"histogram_boxplot(train_data, \"Age\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:29.904779Z","iopub.execute_input":"2022-06-05T13:59:29.905440Z","iopub.status.idle":"2022-06-05T13:59:30.506270Z","shell.execute_reply.started":"2022-06-05T13:59:29.905401Z","shell.execute_reply":"2022-06-05T13:59:30.505390Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"There is a spike in people with age close to zero, which means there are a good number of infants on the ship.\n\nGiven that the age distribution is generally a normal distribution with most people being in the 20s and 30s, and the mean being close to the median, I'm comfortable with replacing all the null age values with 30.","metadata":{}},{"cell_type":"code","source":"train_data['Age'].replace(np.nan,30,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:30.507658Z","iopub.execute_input":"2022-06-05T13:59:30.508578Z","iopub.status.idle":"2022-06-05T13:59:30.514490Z","shell.execute_reply.started":"2022-06-05T13:59:30.508528Z","shell.execute_reply":"2022-06-05T13:59:30.513582Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:30.515774Z","iopub.execute_input":"2022-06-05T13:59:30.516307Z","iopub.status.idle":"2022-06-05T13:59:30.534641Z","shell.execute_reply.started":"2022-06-05T13:59:30.516259Z","shell.execute_reply":"2022-06-05T13:59:30.533460Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"##### Does Port of Embarkation have an impact of survival status of a person?","metadata":{}},{"cell_type":"code","source":"stacked_barplot(train_data, \"Embarked\", \"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:30.536118Z","iopub.execute_input":"2022-06-05T13:59:30.536446Z","iopub.status.idle":"2022-06-05T13:59:30.790512Z","shell.execute_reply.started":"2022-06-05T13:59:30.536414Z","shell.execute_reply":"2022-06-05T13:59:30.789543Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"labeled_barplot(train_data, \"Embarked\", perc=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:30.791965Z","iopub.execute_input":"2022-06-05T13:59:30.792636Z","iopub.status.idle":"2022-06-05T13:59:30.998175Z","shell.execute_reply.started":"2022-06-05T13:59:30.792587Z","shell.execute_reply":"2022-06-05T13:59:30.997158Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"It appears like people with Cherbourg as the Port of Embarkation were more likely to survive compared to Queenstown and Southampton. \n\nGiven that we only have 2 null values in the train data and Southampton encompasses almost three-fourths of the train data, I'm ok with replacing the null values with Southampton.","metadata":{}},{"cell_type":"code","source":"train_data['Embarked'].replace(np.nan,\"S\",inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:30.999589Z","iopub.execute_input":"2022-06-05T13:59:31.000069Z","iopub.status.idle":"2022-06-05T13:59:31.006494Z","shell.execute_reply.started":"2022-06-05T13:59:31.000027Z","shell.execute_reply":"2022-06-05T13:59:31.005796Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"##### Does sex have an impact on the survival rate of a person?","metadata":{}},{"cell_type":"code","source":"stacked_barplot(train_data, \"Sex\", \"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:31.008183Z","iopub.execute_input":"2022-06-05T13:59:31.009044Z","iopub.status.idle":"2022-06-05T13:59:31.253899Z","shell.execute_reply.started":"2022-06-05T13:59:31.009003Z","shell.execute_reply":"2022-06-05T13:59:31.252813Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"labeled_barplot(train_data, \"Sex\", perc=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:31.255432Z","iopub.execute_input":"2022-06-05T13:59:31.256069Z","iopub.status.idle":"2022-06-05T13:59:31.423745Z","shell.execute_reply.started":"2022-06-05T13:59:31.256023Z","shell.execute_reply":"2022-06-05T13:59:31.422683Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"The distribution of sex on the Titanic was about two-thirds male and one-third female. However, the survival percentage was a lot higher for females than males. \nThis is consistent with the movie, where women and children were prioritized escape!","metadata":{}},{"cell_type":"markdown","source":"##### How many attendees on the ship had others in their party?","metadata":{}},{"cell_type":"code","source":"histogram_boxplot(train_data, \"SibSp\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:31.425584Z","iopub.execute_input":"2022-06-05T13:59:31.425920Z","iopub.status.idle":"2022-06-05T13:59:31.899913Z","shell.execute_reply.started":"2022-06-05T13:59:31.425874Z","shell.execute_reply":"2022-06-05T13:59:31.898941Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"histogram_boxplot(train_data, \"Parch\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:31.901348Z","iopub.execute_input":"2022-06-05T13:59:31.901728Z","iopub.status.idle":"2022-06-05T13:59:32.308138Z","shell.execute_reply.started":"2022-06-05T13:59:31.901682Z","shell.execute_reply":"2022-06-05T13:59:32.307294Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"distribution_plot_wrt_target(train_data,\"SibSp\",\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:32.309347Z","iopub.execute_input":"2022-06-05T13:59:32.309593Z","iopub.status.idle":"2022-06-05T13:59:33.257387Z","shell.execute_reply.started":"2022-06-05T13:59:32.309563Z","shell.execute_reply":"2022-06-05T13:59:33.256483Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"distribution_plot_wrt_target(train_data,\"Parch\",\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:33.258588Z","iopub.execute_input":"2022-06-05T13:59:33.258826Z","iopub.status.idle":"2022-06-05T13:59:34.037496Z","shell.execute_reply.started":"2022-06-05T13:59:33.258795Z","shell.execute_reply":"2022-06-05T13:59:34.036817Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Most attendees on the ship were alone. This is likely skewed to 0 because the data set includes crew members as well.\n\nThere appears to be no correlation between number of siblings/spouses and survival rate.\nHowever, people with parents/children seem to be more likely to survive.\n\n\n\nLet's briefly assume that a crew member could be identified by if the Fare was 0. Would this change the distribution and survival status?","metadata":{}},{"cell_type":"code","source":"histogram_boxplot(train_data[train_data[\"Fare\"]!=0], \"SibSp\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:34.038634Z","iopub.execute_input":"2022-06-05T13:59:34.039665Z","iopub.status.idle":"2022-06-05T13:59:34.484624Z","shell.execute_reply.started":"2022-06-05T13:59:34.039618Z","shell.execute_reply":"2022-06-05T13:59:34.483545Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"histogram_boxplot(train_data[train_data[\"Fare\"]!=0], \"Parch\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:34.485990Z","iopub.execute_input":"2022-06-05T13:59:34.486248Z","iopub.status.idle":"2022-06-05T13:59:35.049409Z","shell.execute_reply.started":"2022-06-05T13:59:34.486217Z","shell.execute_reply":"2022-06-05T13:59:35.048418Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"distribution_plot_wrt_target(train_data[train_data[\"Fare\"]!=0],\"SibSp\",\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:35.050986Z","iopub.execute_input":"2022-06-05T13:59:35.051502Z","iopub.status.idle":"2022-06-05T13:59:35.945478Z","shell.execute_reply.started":"2022-06-05T13:59:35.051430Z","shell.execute_reply":"2022-06-05T13:59:35.944538Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"distribution_plot_wrt_target(train_data[train_data[\"Fare\"]!=0],\"Parch\",\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:35.946750Z","iopub.execute_input":"2022-06-05T13:59:35.946999Z","iopub.status.idle":"2022-06-05T13:59:36.722623Z","shell.execute_reply.started":"2022-06-05T13:59:35.946969Z","shell.execute_reply":"2022-06-05T13:59:36.721768Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"After removing rows where Fare was zero (because they were assumed as crew members), there didn't appear to be a major change in the distribution and survival status.\n\nBeyond Fare = zero, is there an correlation for Fare with survival rate?\n\n","metadata":{}},{"cell_type":"code","source":"histogram_boxplot(train_data, \"Fare\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:36.724041Z","iopub.execute_input":"2022-06-05T13:59:36.724271Z","iopub.status.idle":"2022-06-05T13:59:37.316506Z","shell.execute_reply.started":"2022-06-05T13:59:36.724243Z","shell.execute_reply":"2022-06-05T13:59:37.315904Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"distribution_plot_wrt_target(train_data,\"Fare\",\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:37.317463Z","iopub.execute_input":"2022-06-05T13:59:37.317890Z","iopub.status.idle":"2022-06-05T13:59:38.219636Z","shell.execute_reply.started":"2022-06-05T13:59:37.317853Z","shell.execute_reply":"2022-06-05T13:59:38.218815Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Generally, those that paid a higher fare were more likely to survive.","metadata":{}},{"cell_type":"markdown","source":"##### Is there a correlation between Pclass and Fare?","metadata":{}},{"cell_type":"code","source":"distribution_plot_wrt_target(train_data,\"Fare\",\"Pclass\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:38.220736Z","iopub.execute_input":"2022-06-05T13:59:38.220947Z","iopub.status.idle":"2022-06-05T13:59:39.310483Z","shell.execute_reply.started":"2022-06-05T13:59:38.220919Z","shell.execute_reply":"2022-06-05T13:59:39.309585Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Yes, people in a ticket class of 1 generally paid a higher fare. People in a ticket class of 3 generally paid the lowest for fare. These may be multicollinear variables.\n\nNow that we got a good understanding and visualization of the data and we cleaned appropriate places, let's prepare the data for modeling!","metadata":{}},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"X_train = train_data.drop([\"Survived\"],axis=1)\ny_train = train_data[\"Survived\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:39.311639Z","iopub.execute_input":"2022-06-05T13:59:39.311945Z","iopub.status.idle":"2022-06-05T13:59:39.317081Z","shell.execute_reply.started":"2022-06-05T13:59:39.311914Z","shell.execute_reply":"2022-06-05T13:59:39.316522Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression Modeling","metadata":{}},{"cell_type":"markdown","source":"Logistic Regression modeling will require a little more data preparation. We need to add a constant and one-hot encode categorical variables.","metadata":{}},{"cell_type":"code","source":"# adding a contstant to X variable\nX_log1 = add_constant(X_train)\n\n# creating dummies\nX_log1 = pd.get_dummies(X_log1, drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:39.317964Z","iopub.execute_input":"2022-06-05T13:59:39.318702Z","iopub.status.idle":"2022-06-05T13:59:39.345194Z","shell.execute_reply.started":"2022-06-05T13:59:39.318659Z","shell.execute_reply":"2022-06-05T13:59:39.344042Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# fitting the model on training set\nlogit1 = sm.Logit(y_train, X_log1.astype(float))\nlg1 = logit1.fit()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:39.347042Z","iopub.execute_input":"2022-06-05T13:59:39.347399Z","iopub.status.idle":"2022-06-05T13:59:39.364215Z","shell.execute_reply.started":"2022-06-05T13:59:39.347365Z","shell.execute_reply":"2022-06-05T13:59:39.362941Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# let's print the logistic regression summary\nprint(lg1.summary())","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:39.365758Z","iopub.execute_input":"2022-06-05T13:59:39.366168Z","iopub.status.idle":"2022-06-05T13:59:39.399223Z","shell.execute_reply.started":"2022-06-05T13:59:39.366122Z","shell.execute_reply":"2022-06-05T13:59:39.398336Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"##### Observations:\n- Embarked_Q, Parch, and Fare have high p-value, so there's likely some multicollinearity at play and it's likely worth removing some of them.","metadata":{}},{"cell_type":"markdown","source":"Before doing further enhancements to this model, let's check the model performance with a confusion matrix.\n\nGenerally, I care about decreasing false positives and false negatives equally. So I will primarily focus on using the F1 score to track model performance too.\n\nWe will also set a default threshold of 0.5.","metadata":{}},{"cell_type":"code","source":"def conf_matrix_and_scores(X_train, y_train, lg, threshold=0.5):\n    \"\"\"\n    Make a confusion matrix and calculate metrics\n    X_train: The X train data set\n    y_train: The y train data set\n    lg: the logistic regression model\n    threshold: threshold to use for the model (default 0.5)\n    \"\"\"\n    pred_train = lg.predict(X_train) > threshold\n    pred_train = np.round(pred_train)\n    cm = confusion_matrix(y_train, pred_train)\n    plt.figure(figsize=(7, 5))\n    sns.heatmap(cm, annot=True, fmt=\"g\")\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Actual Values\")\n    plt.show()\n    \n    print(\"Accuracy on training set : \", accuracy_score(y_train, pred_train))\n    print(\"Recall on training set: \", recall_score(y_train, pred_train))\n    print(\"Precision on training set: \", precision_score(y_train, pred_train))\n    print(\"F1 score on training set: \", f1_score(y_train, pred_train))","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:39.400574Z","iopub.execute_input":"2022-06-05T13:59:39.401084Z","iopub.status.idle":"2022-06-05T13:59:39.412276Z","shell.execute_reply.started":"2022-06-05T13:59:39.401038Z","shell.execute_reply":"2022-06-05T13:59:39.411379Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"conf_matrix_and_scores(X_log1, y_train, lg1)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:39.413735Z","iopub.execute_input":"2022-06-05T13:59:39.414258Z","iopub.status.idle":"2022-06-05T13:59:39.694188Z","shell.execute_reply.started":"2022-06-05T13:59:39.414214Z","shell.execute_reply":"2022-06-05T13:59:39.693546Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"An F1 score of 73% is good. But there are a good number of false positives and false negatives. Let's handle that better through some data cleaning. Starting with multicollinearity calculations.","metadata":{}},{"cell_type":"code","source":"# let's check the VIF of the predictors\nvif_series = pd.Series(\n    [variance_inflation_factor(X_log1.values, i) for i in range(X_log1.shape[1])],\n    index=X_log1.columns,\n    dtype=float,\n)\nprint(\"VIF values: \\n\\n{}\\n\".format(vif_series))","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:39.700551Z","iopub.execute_input":"2022-06-05T13:59:39.701033Z","iopub.status.idle":"2022-06-05T13:59:39.719602Z","shell.execute_reply.started":"2022-06-05T13:59:39.700979Z","shell.execute_reply":"2022-06-05T13:59:39.717893Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, there is no strong multicollinearity in the data. I expected at least Pclass or Fare to be related.\n\nI will still remove the Embarked_Q column and make a new model.","metadata":{}},{"cell_type":"code","source":"X_log2 = X_log1.drop([\"Embarked_Q\"],axis=1)\nlogit2 = sm.Logit(y_train, X_log2.astype(float))\nlg2 = logit2.fit()\nprint(lg2.summary())","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:39.721205Z","iopub.execute_input":"2022-06-05T13:59:39.721573Z","iopub.status.idle":"2022-06-05T13:59:39.767528Z","shell.execute_reply.started":"2022-06-05T13:59:39.721529Z","shell.execute_reply":"2022-06-05T13:59:39.766514Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"conf_matrix_and_scores(X_log2, y_train, lg2)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:39.773101Z","iopub.execute_input":"2022-06-05T13:59:39.774039Z","iopub.status.idle":"2022-06-05T13:59:40.057107Z","shell.execute_reply.started":"2022-06-05T13:59:39.773975Z","shell.execute_reply":"2022-06-05T13:59:40.056247Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"As expected, p-values are still intact.\n\nLet's go through our metrics calculations of this model.","metadata":{}},{"cell_type":"markdown","source":"No major change in metrics.\n\nParch has the highest p-value, so I will remove that now and model again.","metadata":{}},{"cell_type":"code","source":"X_log3 = X_log2.drop([\"Parch\"],axis=1)\nlogit = sm.Logit(y_train, X_log3.astype(float))\nlg3 = logit.fit()\nprint(lg3.summary())","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:40.058717Z","iopub.execute_input":"2022-06-05T13:59:40.059044Z","iopub.status.idle":"2022-06-05T13:59:40.098814Z","shell.execute_reply.started":"2022-06-05T13:59:40.058962Z","shell.execute_reply":"2022-06-05T13:59:40.097764Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"conf_matrix_and_scores(X_log3, y_train, lg3)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:40.100403Z","iopub.execute_input":"2022-06-05T13:59:40.101009Z","iopub.status.idle":"2022-06-05T13:59:40.333587Z","shell.execute_reply.started":"2022-06-05T13:59:40.100953Z","shell.execute_reply":"2022-06-05T13:59:40.332652Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"No major change in metrics.\n\nFare sill has a high p-value, so I will remove that now and model again.","metadata":{}},{"cell_type":"code","source":"X_log4 = X_log3.drop([\"Fare\"],axis=1)\nlogit = sm.Logit(y_train, X_log4.astype(float))\nlg4 = logit.fit()\nprint(lg4.summary())","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:40.335091Z","iopub.execute_input":"2022-06-05T13:59:40.335410Z","iopub.status.idle":"2022-06-05T13:59:40.373958Z","shell.execute_reply.started":"2022-06-05T13:59:40.335361Z","shell.execute_reply":"2022-06-05T13:59:40.372909Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"conf_matrix_and_scores(X_log4, y_train, lg4)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:40.375485Z","iopub.execute_input":"2022-06-05T13:59:40.376436Z","iopub.status.idle":"2022-06-05T13:59:40.653944Z","shell.execute_reply.started":"2022-06-05T13:59:40.376366Z","shell.execute_reply":"2022-06-05T13:59:40.653197Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"We will keep lg4 as our logistic regression model. Let's intrepret what this model tells us.\n\n* The coefficients of the logistic regression model are in terms of log(odd), to find the odds we have to take the exponential of the coefficients. \n* Therefore, **odds =  exp(b)**\n* The percentage change in odds is given as **odds = (exp(b) - 1) * 100**","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:37:37.410852Z","iopub.execute_input":"2022-05-29T20:37:37.411127Z","iopub.status.idle":"2022-05-29T20:37:37.451304Z","shell.execute_reply.started":"2022-05-29T20:37:37.411088Z","shell.execute_reply":"2022-05-29T20:37:37.450505Z"}}},{"cell_type":"code","source":"# converting coefficients to odds\nodds = np.exp(lg4.params)\n\n# adding the odds to a dataframe\npd.DataFrame(odds, X_log4.columns, columns=[\"odds\"]).T","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:40.654967Z","iopub.execute_input":"2022-06-05T13:59:40.655672Z","iopub.status.idle":"2022-06-05T13:59:40.670247Z","shell.execute_reply.started":"2022-06-05T13:59:40.655627Z","shell.execute_reply":"2022-06-05T13:59:40.669539Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# finding the percentage change\nperc_change_odds = (np.exp(lg4.params) - 1) * 100\n\n# adding the change_odds% to a dataframe\npd.DataFrame(perc_change_odds, X_log4.columns, columns=[\"change_odds%\"]).T","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:40.671612Z","iopub.execute_input":"2022-06-05T13:59:40.672065Z","iopub.status.idle":"2022-06-05T13:59:40.690731Z","shell.execute_reply.started":"2022-06-05T13:59:40.672022Z","shell.execute_reply":"2022-06-05T13:59:40.689783Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"##### Coefficient Interpretations:\n\n- \"Pclass\": Holding all other features constant, a unit change in Pclass will decrease the odds of survival by 68.80%\n- \"Age\": Holding all other features constant, a unit change in Age will decrease the odds of survival by 3.90%\n- \"SibSp\": Holding all other features constant, a unit change in the number of siblings our spouses in your part will decrease the odds of survival by 28.46%\n- \"Sex\": Holding all other features constant, being a male will decrease the odds of survival by 93.28%.\n- \"Embarked\": Holding all other features constant, embarking from Southampton will decrease the odds of survival by 35.87%.","metadata":{}},{"cell_type":"markdown","source":"Let's see if we can enhance the model further with another threshold value.","metadata":{}},{"cell_type":"code","source":"logit_roc_auc_train = roc_auc_score(y_train, lg4.predict(X_log4))\nfpr, tpr, thresholds = roc_curve(y_train, lg4.predict(X_log4))\nplt.figure(figsize=(7, 5))\nplt.plot(fpr, tpr, label=\"Logistic Regression (area = %0.2f)\" % logit_roc_auc_train)\nplt.plot([0, 1], [0, 1], \"r--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver operating characteristic\")\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:40.692227Z","iopub.execute_input":"2022-06-05T13:59:40.692531Z","iopub.status.idle":"2022-06-05T13:59:40.905982Z","shell.execute_reply.started":"2022-06-05T13:59:40.692498Z","shell.execute_reply":"2022-06-05T13:59:40.905382Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"y_scores = lg4.predict(X_log4)\nprec, rec, tre = precision_recall_curve(y_train, y_scores)\n\n\ndef plot_prec_recall_vs_tresh(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"precision\")\n    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.ylim([0, 1])\n\n\nplt.figure(figsize=(10, 7))\nplot_prec_recall_vs_tresh(prec, rec, tre)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:40.907255Z","iopub.execute_input":"2022-06-05T13:59:40.907705Z","iopub.status.idle":"2022-06-05T13:59:41.126490Z","shell.execute_reply.started":"2022-06-05T13:59:40.907667Z","shell.execute_reply":"2022-06-05T13:59:41.125564Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"0.5 does seem to be the ideal value for the threshold. We'll keep as is.\n\nNow let's explore a decision tree model!","metadata":{}},{"cell_type":"markdown","source":"# Decision Tree Modeling","metadata":{}},{"cell_type":"code","source":"# creating dummies\nX_dt = pd.get_dummies(X_train, drop_first=True)\n\ndt1 = DecisionTreeClassifier(random_state=1)\ndt1.fit(X_dt, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:41.127859Z","iopub.execute_input":"2022-06-05T13:59:41.128702Z","iopub.status.idle":"2022-06-05T13:59:41.149784Z","shell.execute_reply.started":"2022-06-05T13:59:41.128655Z","shell.execute_reply":"2022-06-05T13:59:41.148923Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"We can actually use the same metric calculation function that was used in logistic regression.","metadata":{}},{"cell_type":"code","source":"conf_matrix_and_scores(X_dt, y_train, dt1)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:41.151014Z","iopub.execute_input":"2022-06-05T13:59:41.151322Z","iopub.status.idle":"2022-06-05T13:59:41.383615Z","shell.execute_reply.started":"2022-06-05T13:59:41.151290Z","shell.execute_reply":"2022-06-05T13:59:41.382550Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"This decision tree model looks great, but a problem with this type of modeling is that it is prone to overfitting.\n\nLet's make another decision tree model with a balanced class weight.","metadata":{}},{"cell_type":"code","source":"dt2 = DecisionTreeClassifier(random_state=1, class_weight=\"balanced\") \ndt2.fit(X_dt, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:41.385501Z","iopub.execute_input":"2022-06-05T13:59:41.385858Z","iopub.status.idle":"2022-06-05T13:59:41.401482Z","shell.execute_reply.started":"2022-06-05T13:59:41.385812Z","shell.execute_reply":"2022-06-05T13:59:41.400525Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"conf_matrix_and_scores(X_dt, y_train, dt2)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:41.403033Z","iopub.execute_input":"2022-06-05T13:59:41.403367Z","iopub.status.idle":"2022-06-05T13:59:41.636423Z","shell.execute_reply.started":"2022-06-05T13:59:41.403326Z","shell.execute_reply":"2022-06-05T13:59:41.635626Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize what this model looks like.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 30))\n\nout = tree.plot_tree(\n    dt2,\n    feature_names=list(X_dt.columns),\n    filled=True,\n    fontsize=9,\n    node_ids=True,\n    class_names=True,\n)\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor(\"black\")\n        arrow.set_linewidth(1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:59:41.637558Z","iopub.execute_input":"2022-06-05T13:59:41.637784Z","iopub.status.idle":"2022-06-05T14:00:05.021914Z","shell.execute_reply.started":"2022-06-05T13:59:41.637756Z","shell.execute_reply":"2022-06-05T14:00:05.020900Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"There is clearly a lot of nodes here, clearly overcomplex and overfitting so it should be pruned.","metadata":{}},{"cell_type":"markdown","source":"First, let's get a review of the variable importances.","metadata":{}},{"cell_type":"code","source":"importances = dt2.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12, 12))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [list(X_dt.columns)[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:00:05.023317Z","iopub.execute_input":"2022-06-05T14:00:05.023570Z","iopub.status.idle":"2022-06-05T14:00:05.442232Z","shell.execute_reply.started":"2022-06-05T14:00:05.023540Z","shell.execute_reply":"2022-06-05T14:00:05.441312Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"Sex, Fare, and Age were the highest contributors to the decision tree model. This is pretty different from the logistic regression model we solidified - let's see if this changes as we prune the decision tree model.","metadata":{}},{"cell_type":"markdown","source":"We'll start with pre-pruning. We will circle through the list of various hyperparameters until we get the one with the best F1 score.","metadata":{}},{"cell_type":"code","source":"# Choose the type of classifier.\nestimator = DecisionTreeClassifier(random_state=1)\n\n# Grid of parameters to choose from\n\nparameters = {\n    \"criterion\": [\"entropy\", \"gini\"],\n    \"max_depth\": np.arange(2, 11, 2),   \n    \"max_leaf_nodes\": [50, 75, 150, 250],\n    \"min_samples_split\": [10, 30, 50, 70],\n    \"min_impurity_decrease\": [0.000001, 0.00001, 0.0001],\n}\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(f1_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)\ngrid_obj = grid_obj.fit(X_dt, y_train)\n\n# Set the clf to the best combination of parameters\nestimator = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nestimator.fit(X_dt, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T14:00:05.443623Z","iopub.execute_input":"2022-06-05T14:00:05.443846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our iterative process showed us that the following would be ideal for a decision tree model:\n* Criterion of entropy\n* Max depth of 10\n* Max leaf node number as 50\n* Min samples needed to split as 30\n* Min impurity decrease as 1e-6","metadata":{}},{"cell_type":"code","source":"conf_matrix_and_scores(X_dt, y_train, estimator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 30))\n\nout = tree.plot_tree(\n    estimator,\n    feature_names=list(X_dt.columns),\n    filled=True,\n    fontsize=9,\n    node_ids=True,\n    class_names=True,\n)\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor(\"black\")\n        arrow.set_linewidth(1)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = estimator.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [list(X_dt.columns)[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pre-pruned model is a lot more simplified. The new F1 score is 81%, which is less than before but also is less prone to overfitting.\nThe pre-pruned model also prioritizes sex the most of all the features.","metadata":{}},{"cell_type":"markdown","source":"##### Cost Complexity Pruning:\n\nLet's visualize the effect of the cost complexity parameter with the decision tree modeling. We will recursively find the node with the weakest link by returning the effective alphas and corresponding total leaf impurities at each step. ","metadata":{}},{"cell_type":"code","source":"clf = DecisionTreeClassifier(random_state=1)\npath = clf.cost_complexity_pruning_path(X_dt, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = pd.DataFrame(path)\npath","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will train a decision tree to use the effective alphas. We'll find the alpha that provides the best F1 score.","metadata":{}},{"cell_type":"code","source":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)\n    clf.fit(X_dt, y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should obviously remove the last value, because a decision tree with only 1 node doesn't provide any information.","metadata":{}},{"cell_type":"code","source":"clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1,figsize=(10,7))\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_train = []\nfor clf in clfs:\n    pred_train=clf.predict(X_dt)\n    values_train=metrics.f1_score(y_train,pred_train)\n    f1_train.append(values_train)\nfig, ax = plt.subplots(figsize=(10,5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"F1 Score\")\nax.set_title(\"F1 Score vs alpha for training set\")\nax.plot(ccp_alphas, f1_train, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above graphs, it looks like an alpha of around 0.004 would be best. Higher than that will cause exponential changes to number of nodes and depth. Lower than that risks basing the model on noise and overfitting.\nTo get the exact number, I will grab from the 76th row of the path dataframe.","metadata":{"execution":{"iopub.status.busy":"2022-05-29T23:01:43.546281Z","iopub.execute_input":"2022-05-29T23:01:43.547059Z","iopub.status.idle":"2022-05-29T23:01:43.552514Z","shell.execute_reply.started":"2022-05-29T23:01:43.547005Z","shell.execute_reply":"2022-05-29T23:01:43.551659Z"}}},{"cell_type":"code","source":"ideal_model=clfs[76]\nprint(ideal_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix_and_scores(X_dt, y_train, ideal_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = ideal_model.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [list(X_dt.columns)[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 30))\n\nout = tree.plot_tree(\n    ideal_model,\n    feature_names=list(X_dt.columns),\n    filled=True,\n    fontsize=9,\n    node_ids=True,\n    class_names=True,\n)\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor(\"black\")\n        arrow.set_linewidth(1)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Model Performance Comparison and Conclusions\n\n- We'll use the post-pruned Decision Tree model for our test data set. This is chosen because it is a simple to understand model (depth of only 3 and only 7 leaf nodes) and yet it provides a solid F1 score of 76%.\n- According to this model, the sex of the person is the strongest identifier for survival status, followed by Pclass. This makes sense because women were generally prioritized escape, and richer people probably had more power to influence prioritizing themselves over others.\n- To make a stronger model, the following can be done for the future:\n** Information about if someone was a guest or on the crew. This likely would be a strong factor, as crew members would have known the requirements to get to safety. This data can likely be gathered from other sources.\n** We removed the Cabin column due to having many null values. However, this could be a strong factor too because those that lives lower in the ship would have had to go through more effort to not drown. We can try to find reliable information for this data in other sources, or model with only the rows that have this information.","metadata":{}},{"cell_type":"markdown","source":"# Running Model on Test Data","metadata":{}},{"cell_type":"markdown","source":"First, we need to clean the test data set in the same way that we did it for the train data set before modeling.\nWe will copy into a new data frame to avoid breaking the test data set.","metadata":{}},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Things to clean:\n- Just like the train set, remove the columns for PassengerId, Name, Ticket, Cabin\n- Replace null values for Age with the value of 30.\n- No need to replace null values in the Embarked column because that already has all values populated.\n- There is one row with a null Fare value. We will replace that with a 0 because that is the mode of the Fare column.\n- One-hot encode the categorical variables.","metadata":{}},{"cell_type":"code","source":"# Remove the PassengerId, Name, Ticket, and Cabin columns\ncols = [\"PassengerId\",\"Name\",\"Ticket\", \"Cabin\"]\nX_testdt=test_data\nfor col in cols:\n    X_testdt = X_testdt.drop([col], axis=1)\n    \n# Handle null values\nX_testdt['Age'].replace(np.nan,30,inplace=True)\nX_testdt['Fare'].replace(np.nan,0,inplace=True)\n    \n# One-hot encode variables\nX_testdt = pd.get_dummies(X_testdt, drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confirm we have a test data set in the correct format and no null values\nX_testdt.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick glance at part of the test data set.\nX_testdt.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can make our submission file!","metadata":{}},{"cell_type":"code","source":"predictions = ideal_model.predict(X_testdt)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}